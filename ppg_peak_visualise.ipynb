{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, db\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# ê¸°ì¡´ Firebase ì•±ì´ ìˆìœ¼ë©´ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì´ˆê¸°í™”\n",
    "if firebase_admin._apps:\n",
    "    firebase_admin.delete_app(firebase_admin.get_app())\n",
    "\n",
    "# Firebase ì•± ì´ˆê¸°í™” (ì‚¬ìš©ìì˜ í‚¤ì™€ URLë¡œ ìˆ˜ì • í•„ìš”)\n",
    "cred = credentials.Certificate(\"hrvdataset-firebase-adminsdk-oof96-2a96d6ac7f.json\")  # Firebase Admin SDK JSON íŒŒì¼\n",
    "firebase_admin.initialize_app(cred, {\"databaseURL\": \"https://hrvdataset-default-rtdb.firebaseio.com/\"})\n",
    "\n",
    "# ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "# ref = db.reference(\"HeartRateData\")\n",
    "# data = ref.get()\n",
    "\n",
    "# wakeup.json íŒŒì¼ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"gosleep.json\", \"r\") as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "\n",
    "# ë°ì´í„° ì •ë¦¬\n",
    "ppg_values = []\n",
    "timestamps = []\n",
    "\n",
    "for key, value in data_json[\"HeartRateData\"].items():\n",
    "    if not value[\"isError\"]:  # ì˜¤ë¥˜ ì—†ëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©\n",
    "        ppg_values.append(value[\"ppgGreen\"])\n",
    "        timestamps.append(pd.to_datetime(value[\"timestamp\"]))\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë ¬\n",
    "df = pd.DataFrame({\"Timestamp\": timestamps, \"PPG\": ppg_values})\n",
    "df = df.sort_values(\"Timestamp\")\n",
    "\n",
    "# ì´ˆë°˜ 2ì´ˆ ë°ì´í„° ì‚­ì œ\n",
    "start_time = df[\"Timestamp\"].iloc[0]\n",
    "df = df[df[\"Timestamp\"] > start_time + pd.Timedelta(seconds=2)]\n",
    "\n",
    "# PPG ì‹ í˜¸ ì²­ì†Œ (ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ 25Hz)\n",
    "fs = 25\n",
    "ppg_cleaned = nk.ppg_clean(df[\"PPG\"], sampling_rate=fs)\n",
    "\n",
    "# í”¼í¬ ê²€ì¶œ í•¨ìˆ˜ (min_y ì¶”ê°€)\n",
    "def find_prominent_peaks(signal, threshold=0.1, min_y=0):\n",
    "    peaks = []\n",
    "    for i in range(1, len(signal) - 1):\n",
    "        if signal[i] > signal[i - 1] and signal[i] > signal[i + 1] and signal[i] > min_y:\n",
    "            left_min = min(signal[max(0, i - 5):i])\n",
    "            right_min = min(signal[i + 1:i + 6])\n",
    "            prominence = signal[i] - max(left_min, right_min)\n",
    "            if prominence > threshold:\n",
    "                peaks.append(i)\n",
    "    return peaks\n",
    "\n",
    "# í”¼í¬ ê²€ì¶œ\n",
    "min_y = 0\n",
    "peaks_indices = find_prominent_peaks(ppg_cleaned, threshold=0.1, min_y=min_y)\n",
    "\n",
    "\n",
    "# í”¼í¬ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ê°’ ì¶”ì¶œ\n",
    "peaks_timestamps = df[\"Timestamp\"].iloc[peaks_indices].values\n",
    "peaks_values = ppg_cleaned[peaks_indices]\n",
    "\n",
    "# RR ê°„ê²© ê³„ì‚° (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)\n",
    "rr_intervals = np.diff(peaks_timestamps).astype('timedelta64[ms]').astype(int)\n",
    "\n",
    "# RR ê°„ê²©ì„ ë‹¤ì‹œ í”¼í¬ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "peaks_from_rr = nk.intervals_to_peaks(rr_intervals)\n",
    "\n",
    "# PPG í”¼í¬ ê²€ì¶œ\n",
    "peaks, info = nk.ppg_peaks(ppg_cleaned, sampling_rate=fs, show=True)\n",
    "\n",
    "# peaks ë°ì´í„°í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ë¥¼ dfì™€ ì¼ì¹˜ì‹œí‚¤ê¸°\n",
    "peaks[\"Timestamp\"] = df[\"Timestamp\"].reset_index(drop=True)\n",
    "\n",
    "# HRV ë„ë©”ì¸ë³„ ë¶„ì„\n",
    "hrv_time_metrics = nk.hrv_time(peaks_from_rr, sampling_rate=1000)\n",
    "hrv_frequency_metrics = nk.hrv_frequency(peaks_from_rr, sampling_rate=1000)\n",
    "hrv_nonlinear_metrics = nk.hrv_nonlinear(peaks_from_rr, sampling_rate=1000)\n",
    "\n",
    "# HRV ì§€í‘œ ì¶œë ¥\n",
    "print(\"HRV Time-Domain Metrics:\")\n",
    "print(hrv_time_metrics)\n",
    "print(\"\\nHRV Frequency-Domain Metrics:\")\n",
    "print(hrv_frequency_metrics)\n",
    "print(\"\\nHRV Non-Linear Metrics:\")\n",
    "print(hrv_nonlinear_metrics)\n",
    "\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df[\"Timestamp\"], ppg_cleaned, label=\"Cleaned PPG\", color=\"blue\")\n",
    "plt.scatter(peaks_timestamps, peaks_values, color=\"red\", label=\"Peaks\", zorder=3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"PPG Signal\")\n",
    "plt.title(\"PPG Signal with Detected Peaks\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, db\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import chi2, f\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import chi2, f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Firebase ì•± ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë“œ\n",
    "# ---------------------------\n",
    "\n",
    "# ê¸°ì¡´ Firebase ì•± ì´ˆê¸°í™” (í•„ìš” ì‹œ)\n",
    "if firebase_admin._apps:\n",
    "    firebase_admin.delete_app(firebase_admin.get_app())\n",
    "\n",
    "# Firebase ì•± ì´ˆê¸°í™” (ì‚¬ìš©ìì˜ í‚¤ì™€ URLë¡œ ìˆ˜ì • í•„ìš”)\n",
    "cred = credentials.Certificate(\"hrvdataset-firebase-adminsdk-oof96-2a96d6ac7f.json\")\n",
    "firebase_admin.initialize_app(cred, {\"databaseURL\": \"https://hrvdataset-default-rtdb.firebaseio.com/\"})\n",
    "\n",
    "# ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "# ref = db.reference(\"HeartRateData\")\n",
    "# data = ref.get()\n",
    "\n",
    "# gosleep.json íŒŒì¼ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (firebaseì™€ ë™ì¼í•œ í˜•ì‹)\n",
    "with open(\"test.json\", \"r\") as file_in:\n",
    "    data_json = json.load(file_in)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. PPG ë°ì´í„° ì²˜ë¦¬ ë° HRV ë¶„ì„ (ê¸°ì¡´ ë°©ì‹)\n",
    "# ---------------------------\n",
    "\n",
    "# ë°ì´í„° ì •ë¦¬ (ì˜¤ë¥˜ ì—†ëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©)\n",
    "ppg_values = []\n",
    "ppg_timestamps = []\n",
    "for key, value in data_json[\"HeartRateData\"].items():\n",
    "    if not value[\"isError\"]:\n",
    "        ppg_values.append(value[\"ppgGreen\"])\n",
    "        ppg_timestamps.append(pd.to_datetime(value[\"timestamp\"]))\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ ì •ë ¬\n",
    "df_ppg = pd.DataFrame({\"Timestamp\": ppg_timestamps, \"PPG\": ppg_values})\n",
    "df_ppg = df_ppg.sort_values(\"Timestamp\")\n",
    "\n",
    "# ì´ˆë°˜ 2ì´ˆ ë°ì´í„° ì‚­ì œ (ì‹œì‘ ì•ˆì •í™”)\n",
    "start_time = df_ppg[\"Timestamp\"].iloc[0]\n",
    "df_ppg = df_ppg[df_ppg[\"Timestamp\"] > start_time + pd.Timedelta(seconds=2)]\n",
    "\n",
    "# PPG ì‹ í˜¸ ì²­ì†Œ (ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ 25Hz)\n",
    "fs = 25\n",
    "ppg_cleaned = nk.ppg_clean(df_ppg[\"PPG\"], sampling_rate=fs)\n",
    "\n",
    "# í”¼í¬ ê²€ì¶œ í•¨ìˆ˜ (min_y ì¶”ê°€)\n",
    "def find_prominent_peaks(signal, threshold=0.1, min_y=0):\n",
    "    peaks = []\n",
    "    for i in range(1, len(signal) - 1):\n",
    "        if signal[i] > signal[i - 1] and signal[i] > signal[i + 1] and signal[i] > min_y:\n",
    "            left_min = min(signal[max(0, i - 5):i])\n",
    "            right_min = min(signal[i + 1:i + 6])\n",
    "            prominence = signal[i] - max(left_min, right_min)\n",
    "            if prominence > threshold:\n",
    "                peaks.append(i)\n",
    "    return peaks\n",
    "\n",
    "# ì „ì²´ ì‹ í˜¸ì— ëŒ€í•´ í”¼í¬ ê²€ì¶œ\n",
    "peaks_indices = find_prominent_peaks(ppg_cleaned, threshold=0.1, min_y=0)\n",
    "peaks_timestamps = df_ppg[\"Timestamp\"].iloc[peaks_indices].values  # numpy array of timestamps\n",
    "peaks_values = ppg_cleaned[peaks_indices]\n",
    "\n",
    "# ì „ì²´ ì‹ í˜¸ ë° í”¼í¬ ì‹œê°í™” (ì˜µì…˜)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_ppg[\"Timestamp\"], ppg_cleaned, label=\"Cleaned PPG\", color=\"blue\")\n",
    "plt.scatter(peaks_timestamps, peaks_values, color=\"red\", label=\"Peaks\", zorder=3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"PPG Signal\")\n",
    "plt.title(\"PPG Signal with Detected Peaks\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Accelerometer ë°ì´í„° ì²˜ë¦¬ ë° ì´ìƒ íƒì§€ (MSPC-PCA)\n",
    "# ---------------------------\n",
    "\n",
    "# Accelerometer ë°ì´í„° ì½ê¸° ë° DataFrame ìƒì„±\n",
    "accel_values = []\n",
    "accel_timestamps = []\n",
    "for key, value in data_json[\"AccelerometerData\"].items():\n",
    "    # ì‹¤ì œ í‚¤ ì´ë¦„(\"ax\", \"ay\", \"az\")ëŠ” ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "    accel_values.append([value[\"ax\"], value[\"ay\"], value[\"az\"]])\n",
    "    accel_timestamps.append(pd.to_datetime(value[\"timestamp\"]))\n",
    "df_accel = pd.DataFrame(accel_values, columns=[\"ax\", \"ay\", \"az\"])\n",
    "df_accel[\"Timestamp\"] = accel_timestamps\n",
    "df_accel = df_accel.sort_values(\"Timestamp\")\n",
    "\n",
    "# PPGì™€ ë™ì¼í•˜ê²Œ ì´ˆë°˜ 2ì´ˆ ë°ì´í„° ì‚­ì œ (ì‹œì‘ ì•ˆì •í™”)\n",
    "start_time = df_accel[\"Timestamp\"].iloc[0]\n",
    "df_accel = df_accel[df_accel[\"Timestamp\"] > start_time + pd.Timedelta(seconds=2)]\n",
    "\n",
    "# ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "df_accel.set_index(\"Timestamp\", inplace=True)\n",
    "\n",
    "# ê° 2ë¶„ ìœˆë„ìš°ë§ˆë‹¤ ê°€ì†ë„ê³„ ë°ì´í„° ëŒ€í‘œê°’ ë„ì¶œ (í‰ê·  vs. ì¤‘ì•™ê°’, CV ê¸°ì¤€)\n",
    "def compute_feature(series):\n",
    "    mean_val = series.mean()\n",
    "    std_val = series.std()\n",
    "    cv = (std_val/mean_val * 100) if mean_val != 0 else 0\n",
    "    return series.mean() if cv < 33 else series.median()\n",
    "\n",
    "accel_features = []\n",
    "accel_intervals = []\n",
    "for window_start, group in df_accel.groupby(pd.Grouper(freq=\"2Min\")):\n",
    "    if group.empty:\n",
    "        continue\n",
    "    window_end = window_start + pd.Timedelta(minutes=2)\n",
    "    feat_x = compute_feature(group[\"ax\"])\n",
    "    feat_y = compute_feature(group[\"ay\"])\n",
    "    feat_z = compute_feature(group[\"az\"])\n",
    "    accel_features.append([feat_x, feat_y, feat_z])\n",
    "    accel_intervals.append((window_start, window_end))\n",
    "df_accel_features = pd.DataFrame(accel_features, columns=[\"feat_x\", \"feat_y\", \"feat_z\"])\n",
    "df_accel_features[\"WindowStart\"] = [interval[0] for interval in accel_intervals]\n",
    "df_accel_features[\"WindowEnd\"] = [interval[1] for interval in accel_intervals]\n",
    "\n",
    "# MSPC-PCA ì´ìƒ íƒì§€: ë‹¨ì¼ ì£¼ì„±ë¶„ PCAë¥¼ ì‚¬ìš©í•´ Hotelling TÂ²ì™€ SPE ê³„ì‚°\n",
    "alpha = 0.05\n",
    "\n",
    "N_accel = len(df_accel_features)\n",
    "X_accel = df_accel_features[[\"feat_x\", \"feat_y\", \"feat_z\"]].values.astype(float)\n",
    "X_accel_scaled = StandardScaler().fit_transform(X_accel)\n",
    "pca_accel = PCA(n_components=1)\n",
    "scores_accel = pca_accel.fit_transform(X_accel_scaled).flatten()\n",
    "variance_accel = pca_accel.explained_variance_[0]\n",
    "T2_accel = (scores_accel**2) / variance_accel\n",
    "ulc_t2_accel = (1 * (N_accel + 1) * (N_accel - 1) / (N_accel * (N_accel - 1))) * f.ppf(1 - alpha, 1, N_accel - 1)\n",
    "X_hat_accel = pca_accel.inverse_transform(scores_accel.reshape(-1,1))\n",
    "SPE_accel = ((X_accel_scaled - X_hat_accel)**2).sum(axis=1)\n",
    "b_accel, v_accel = SPE_accel.mean(), SPE_accel.var()\n",
    "df_chi_accel = (2 * b_accel * b_accel) / v_accel\n",
    "ulc_spe_accel = (v_accel / (2 * b_accel)) * chi2.ppf(1 - alpha, df_chi_accel)\n",
    "\n",
    "df_accel_features[\"T2\"] = T2_accel\n",
    "df_accel_features[\"SPE\"] = SPE_accel\n",
    "df_accel_features[\"OOC_T2\"] = T2_accel > ulc_t2_accel\n",
    "df_accel_features[\"OOC_SPE\"] = SPE_accel > ulc_spe_accel\n",
    "df_accel_features[\"Anomaly\"] = df_accel_features[\"OOC_T2\"] | df_accel_features[\"OOC_SPE\"]\n",
    "\n",
    "# ì´ìƒ êµ¬ê°„(2ë¶„ ìœˆë„ìš°) ê¸°ë¡: ì´ìƒì¹˜ì¸ ìœˆë„ìš°ì˜ ì‹œì‘, ì¢…ë£Œ ì‹œê°„\n",
    "accel_anomaly_intervals = []\n",
    "for index, row in df_accel_features.iterrows():\n",
    "    if row[\"Anomaly\"]:\n",
    "        accel_anomaly_intervals.append((row[\"WindowStart\"], row[\"WindowEnd\"]))\n",
    "print(\"Accelerometer anomaly intervals:\", accel_anomaly_intervals)\n",
    "\n",
    "# Control Chart ì‹œê°í™” â€” Hotellingâ€™s TÂ² (Accelerometer)\n",
    "plt.figure()\n",
    "plt.plot(df_accel_features[\"WindowStart\"], T2_accel, marker=\"o\")\n",
    "plt.axhline(ulc_t2_accel, linestyle=\"--\", color=\"red\", label=\"ULC TÂ²\")\n",
    "plt.title(\"Accelerometer Domain â€” Hotellingâ€™s TÂ²\")\n",
    "plt.xlabel(\"Window Start\")\n",
    "plt.ylabel(\"TÂ²\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Control Chart ì‹œê°í™” â€” SPE (Accelerometer)\n",
    "plt.figure()\n",
    "plt.plot(df_accel_features[\"WindowStart\"], SPE_accel, marker=\"o\")\n",
    "plt.axhline(ulc_spe_accel, linestyle=\"--\", color=\"red\", label=\"ULC SPE\")\n",
    "plt.title(\"Accelerometer Domain â€” SPE\")\n",
    "plt.xlabel(\"Window Start\")\n",
    "plt.ylabel(\"SPE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì´ìƒ êµ¬ê°„ ì¶œë ¥ (Accelerometer)\n",
    "ooc_accel = df_accel_features[df_accel_features[\"OOC_T2\"] | df_accel_features[\"OOC_SPE\"]]\n",
    "print(\"\\n=== Accelerometer Domain Outâ€‘ofâ€‘Control Segments ===\")\n",
    "print(ooc_accel[[\"WindowStart\", \"WindowEnd\", \"OOC_T2\", \"OOC_SPE\"]])\n",
    "\n",
    "# ---------------------------\n",
    "# 3. PPG ë°ì´í„° í•„í„°ë§: accelerometer ì´ìƒ êµ¬ê°„ ì œê±°\n",
    "# ---------------------------\n",
    "def is_in_anomaly(timestamp, anomaly_intervals):\n",
    "    for start, end in anomaly_intervals:\n",
    "        if start <= timestamp <= end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df_ppg_filtered = df_ppg[~df_ppg[\"Timestamp\"].apply(lambda t: is_in_anomaly(t, accel_anomaly_intervals))]\n",
    "print(f\"Original PPG count: {len(df_ppg)}, Filtered count: {len(df_ppg_filtered)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. PPG ì‹ í˜¸ ì „ì²˜ë¦¬: í•„í„°ë§ëœ PPG ë°ì´í„°ë¡œ HRV ë° MSPC-PCA ì´ìƒ íƒì§€ ìˆ˜í–‰\n",
    "# ---------------------------\n",
    "ppg_cleaned_filtered = nk.ppg_clean(df_ppg_filtered[\"PPG\"], sampling_rate=fs)\n",
    "peaks_indices_filtered = find_prominent_peaks(ppg_cleaned_filtered, threshold=0.1, min_y=0)\n",
    "peaks_timestamps_filtered = df_ppg_filtered[\"Timestamp\"].iloc[peaks_indices_filtered].values\n",
    "peaks_values_filtered = ppg_cleaned_filtered[peaks_indices_filtered]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df_ppg_filtered[\"Timestamp\"], ppg_cleaned_filtered, label=\"Cleaned PPG (Filtered)\", color=\"blue\")\n",
    "plt.scatter(peaks_timestamps_filtered, peaks_values_filtered, color=\"red\", label=\"Peaks (Filtered)\", zorder=3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"PPG Signal\")\n",
    "plt.title(\"PPG Signal with Detected Peaks (Filtered by Accelerometer Anomalies)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. í”¼í¬ ê¸°ì¤€ìœ¼ë¡œ 2ë¶„ ë‹¨ìœ„ HRV ë¶„ì„\n",
    "# ----------------------------------------------------\n",
    "hrv_results = []\n",
    "i = 0\n",
    "peaks_ts = peaks_timestamps_filtered\n",
    "while i < len(peaks_ts):\n",
    "    seg_start = peaks_ts[i]\n",
    "    seg_end = seg_start + pd.Timedelta(minutes=2)\n",
    "    segment_peaks = []\n",
    "    while i < len(peaks_ts) and peaks_ts[i] < seg_end:\n",
    "        segment_peaks.append(peaks_indices_filtered[i])\n",
    "        i += 1\n",
    "    if len(segment_peaks) < 2:  \n",
    "        print(f\"êµ¬ê°„ {seg_start} ~ {seg_end} ì—ëŠ” í”¼í¬ê°€ ë¶€ì¡±í•˜ì—¬ HRV ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        continue\n",
    "    \n",
    "    # HR ì‹œë¦¬ì¦ˆ ìƒì„± (bpm)\n",
    "    segment_times = peaks_ts[i - len(segment_peaks):i]  # datetime64 array\n",
    "    rri = np.diff(segment_times).astype(\"timedelta64[ms]\").astype(int)  # ms ë‹¨ìœ„ RR intervals\n",
    "    hr_series = 60000 / rri\n",
    "    \n",
    "    hrv_time = nk.hrv_time(segment_peaks, sampling_rate=25)\n",
    "    hrv_time_metrics = {\n",
    "        \"mean_nni\": hrv_time[\"HRV_MeanNN\"].iloc[0],\n",
    "        \"median_nni\": hrv_time[\"HRV_MedianNN\"].iloc[0],\n",
    "        \"range_nni\": hrv_time[\"HRV_MaxNN\"].iloc[0] - hrv_time[\"HRV_MinNN\"].iloc[0],\n",
    "        \"sdnn\": hrv_time[\"HRV_SDNN\"].iloc[0],\n",
    "        \"sdsd\": hrv_time[\"HRV_SDSD\"].iloc[0],\n",
    "        \"rmssd\": hrv_time[\"HRV_RMSSD\"].iloc[0],\n",
    "        \"nni_50\": int(np.sum(np.abs(np.diff(rri)) > 50)),\n",
    "        \"pnni_50\": hrv_time[\"HRV_pNN50\"].iloc[0],\n",
    "        \"nni_20\": int(np.sum(np.abs(np.diff(rri)) > 20)),\n",
    "        \"pnni_20\": hrv_time[\"HRV_pNN20\"].iloc[0],\n",
    "        \"cvsd\": hrv_time[\"HRV_CVSD\"].iloc[0],\n",
    "        \"cvnni\": hrv_time[\"HRV_CVNN\"].iloc[0],\n",
    "        \"mean_hr\": np.nanmean(hr_series),\n",
    "        \"min_hr\": np.nanmin(hr_series),\n",
    "        \"max_hr\": np.nanmax(hr_series),\n",
    "        \"std_hr\": np.nanstd(hr_series, ddof=1),\n",
    "    }\n",
    "    \n",
    "    # ì£¼íŒŒìˆ˜ ì˜ì—­ HRV ê³„ì‚°\n",
    "    hrv_freq = nk.hrv_frequency(segment_peaks, sampling_rate=25, normalize=False)\n",
    "    if not hrv_freq.empty:\n",
    "        hrv_freq_metrics = {\n",
    "            \"power_vlf\": hrv_freq[\"HRV_VLF\"].iloc[0],\n",
    "            \"power_lf\":  hrv_freq[\"HRV_LF\"].iloc[0],\n",
    "            \"power_hf\":  hrv_freq[\"HRV_HF\"].iloc[0],\n",
    "            \"total_power\": hrv_freq[\"HRV_TP\"].iloc[0],\n",
    "            \"lf_hf_ratio\": hrv_freq[\"HRV_LFHF\"].iloc[0]\n",
    "        }\n",
    "    else:\n",
    "        hrv_freq_metrics = {k: np.nan for k in [\"power_vlf\",\"power_lf\",\"power_hf\",\"total_power\",\"lf_hf_ratio\"]}\n",
    "    \n",
    "    # ë¹„ì„ í˜• ì˜ì—­ ì£¼íŒŒìˆ˜ ê³„ì‚°\n",
    "    hrv_nonlinear = nk.hrv_nonlinear(segment_peaks, sampling_rate=25)\n",
    "    if not hrv_nonlinear.empty:\n",
    "        hrv_nonlinear_metrics = {\n",
    "            \"csi\":            hrv_nonlinear[\"HRV_CSI\"].iloc[0],\n",
    "            \"cvi\":            hrv_nonlinear[\"HRV_CVI\"].iloc[0],\n",
    "            \"modified_csi\":   hrv_nonlinear[\"HRV_CSI_Modified\"].iloc[0],\n",
    "            \"sampen\":         hrv_nonlinear[\"HRV_SampEn\"].iloc[0],\n",
    "        }\n",
    "    else:\n",
    "        hrv_nonlinear_metrics = {k: np.nan for k in [\"csi\",\"cvi\",\"modified_csi\",\"sampen\"]}\n",
    "    \n",
    "    \n",
    "    result = {\n",
    "        \"Segment Start\": seg_start,\n",
    "        \"Segment End\": seg_end,\n",
    "        \"HRV Time Metrics\": hrv_time_metrics,\n",
    "        \"HRV Frequency Metrics\": hrv_freq_metrics,\n",
    "        \"HRV Nonlinear Metrics\": hrv_nonlinear_metrics\n",
    "    }\n",
    "    hrv_results.append(result)\n",
    "\n",
    "# HRV ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©í•˜ê¸°\n",
    "results_list = []\n",
    "for res in hrv_results:\n",
    "    row = {\n",
    "        \"Segment Start\": res[\"Segment Start\"],\n",
    "        \"Segment End\":   res[\"Segment End\"]\n",
    "    }\n",
    "    # Time-domain dict â†’ Time_ ì ‘ë‘ì–´\n",
    "    for key, val in res[\"HRV Time Metrics\"].items():\n",
    "        row[f\"Time_{key}\"] = val\n",
    "\n",
    "    # Frequency-domain dict â†’ Freq_ ì ‘ë‘ì–´\n",
    "    for key, val in res[\"HRV Frequency Metrics\"].items():\n",
    "        row[f\"Freq_{key}\"] = val\n",
    "\n",
    "    # Nonlinear-domain dict â†’ Nonlinear_ ì ‘ë‘ì–´\n",
    "    for key, val in res[\"HRV Nonlinear Metrics\"].items():\n",
    "        row[f\"Nonlinear_{key}\"] = val\n",
    "\n",
    "    results_list.append(row)\n",
    "\n",
    "df_hrv_results = pd.DataFrame(results_list)\n",
    "\n",
    "# --- ë”•ì…”ë„ˆë¦¬ â†’ DataFrame ìƒì„± ---\n",
    "df_segment = pd.DataFrame({\n",
    "    \"Time\": pd.Series(res[\"HRV Time Metrics\"]),\n",
    "    \"Frequency\": pd.Series(res[\"HRV Frequency Metrics\"]),\n",
    "    \"Nonlinear\": pd.Series(res[\"HRV Nonlinear Metrics\"])\n",
    "})\n",
    "\n",
    "\n",
    "start_str = pd.to_datetime(seg_start).strftime(\"%H:%M:%S\")\n",
    "end_str   = pd.to_datetime(seg_end).strftime(\"%H:%M:%S\")\n",
    "# ì¶œë ¥\n",
    "print(f\"\\n===== Segment {start_str} â†’ {end_str} =====\")\n",
    "print(df_segment.round(3))\n",
    "\n",
    "\n",
    "# HRV ê²°ê³¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ë ¤ë©´ ì—¬ê¸° ì‚¬ìš©ìš©\n",
    "df_hrv_results.to_csv(\"hrv_results.csv\", index=False)\n",
    "print(\"HRV ê²°ê³¼ê°€ hrv_results.csv íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. MSPC-PCA ì´ìƒ íƒì§€ on HRV (ê° ë„ë©”ì¸ë³„)\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "\n",
    "N = len(df_hrv_results)\n",
    "\n",
    "# ë„ë©”ì¸ë³„ ì»¬ëŸ¼ ë§¤í•‘\n",
    "domains = {\n",
    "    \"Time\": df_hrv_results.filter(regex=\"^Time_\").columns,\n",
    "    \"Freq\": df_hrv_results.filter(regex=\"^Freq_\").columns,\n",
    "    \"Nonlinear\": df_hrv_results.filter(regex=\"^Nonlinear_\").columns\n",
    "}\n",
    "\n",
    "for domain, cols in domains.items():\n",
    "    # 1) ë°ì´í„° ì¤€ë¹„ ë° í‘œì¤€í™”\n",
    "    X = df_hrv_results[cols].fillna(0).values.astype(float)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # 2) PCA (ë‹¨ì¼ ì£¼ì„±ë¶„)\n",
    "    pca = PCA(n_components=1)\n",
    "    scores = pca.fit_transform(X_scaled).flatten()\n",
    "    variance = pca.explained_variance_[0]\n",
    "\n",
    "    # 3) Hotellingâ€™s TÂ² ê³„ì‚°\n",
    "    T2 = (scores ** 2) / variance\n",
    "    ulc_t2 = (1 * (N + 1) * (N - 1) / (N * (N - 1))) * f.ppf(1 - alpha, 1, N - 1)\n",
    "\n",
    "    # 4) SPE (Qâ€‘statistic) ê³„ì‚°\n",
    "    X_hat = pca.inverse_transform(scores.reshape(-1, 1))\n",
    "    SPE = ((X_scaled - X_hat) ** 2).sum(axis=1)\n",
    "    b, v = SPE.mean(), SPE.var()\n",
    "    df_chi = (2 * b * b) / v\n",
    "    ulc_spe = (v / (2 * b)) * chi2.ppf(1 - alpha, df_chi)\n",
    "\n",
    "    # 5) ê²°ê³¼ DataFrame ì¶”ê°€\n",
    "    df_hrv_results[f\"{domain}_T2\"] = T2\n",
    "    df_hrv_results[f\"{domain}_SPE\"] = SPE\n",
    "    df_hrv_results[f\"{domain}_OOC_T2\"] = T2 > ulc_t2\n",
    "    df_hrv_results[f\"{domain}_OOC_SPE\"] = SPE > ulc_spe\n",
    "\n",
    "    # 6) Control Chart ì‹œê°í™” â€” Hotellingâ€™s TÂ²\n",
    "    plt.figure()\n",
    "    plt.plot(df_hrv_results[\"Segment Start\"], T2, marker=\"o\")\n",
    "    plt.axhline(ulc_t2, linestyle=\"--\")\n",
    "    plt.title(f\"{domain} Domain â€” Hotellingâ€™s TÂ²\")\n",
    "    plt.xlabel(\"Segment Start\")\n",
    "    plt.ylabel(\"TÂ²\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 7) Control Chart ì‹œê°í™” â€” SPE\n",
    "    plt.figure()\n",
    "    plt.plot(df_hrv_results[\"Segment Start\"], SPE, marker=\"o\")\n",
    "    plt.axhline(ulc_spe, linestyle=\"--\")\n",
    "    plt.title(f\"{domain} Domain â€” SPE\")\n",
    "    plt.xlabel(\"Segment Start\")\n",
    "    plt.ylabel(\"SPE\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 8) ì´ìƒêµ¬ê°„ ì¶œë ¥\n",
    "    ooc = df_hrv_results[df_hrv_results[f\"{domain}_OOC_T2\"] | df_hrv_results[f\"{domain}_OOC_SPE\"]]\n",
    "    print(f\"\\n=== {domain} Domain Outâ€‘ofâ€‘Control Segments ===\")\n",
    "    print(ooc[[\"Segment Start\",\"Segment End\",f\"{domain}_OOC_T2\",f\"{domain}_OOC_SPE\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neourokit2ë¥¼ ì‚¬ìš©í•´ peakë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ peakëŠ” ì •í™•í•˜ì§€ë§Œ íƒì§€í•˜ì§€ ëª»í•˜ëŠ” peakê°€ ìˆìŒ\n",
    "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, db\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if firebase_admin._apps:\n",
    "    firebase_admin.delete_app(firebase_admin.get_app())\n",
    "\n",
    "# ğŸ”¹ Firebase ì•± ìƒˆë¡œ ì´ˆê¸°í™”\n",
    "cred = credentials.Certificate(\"hrvdataset-firebase-adminsdk-oof96-2a96d6ac7f.json\")  # Firebase Admin SDK JSON íŒŒì¼\n",
    "firebase_admin.initialize_app(cred, {\"databaseURL\": \"https://hrvdataset-default-rtdb.firebaseio.com/\"})\n",
    "\n",
    "ref = db.reference(\"HeartRateData\")\n",
    "data = ref.get()\n",
    "\n",
    "# ë°ì´í„° ì •ë¦¬\n",
    "ppg_values = []\n",
    "timestamps = []\n",
    "\n",
    "for key, value in data.items():\n",
    "    if not value[\"isError\"]:  # ì˜¤ë¥˜ ì—†ëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©\n",
    "        ppg_values.append(value[\"ppgGreen\"])\n",
    "        timestamps.append(pd.to_datetime(value[\"timestamp\"]))\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df = pd.DataFrame({\"Timestamp\": timestamps, \"PPG\": ppg_values})\n",
    "df = df.sort_values(\"Timestamp\")  # ì‹œê°„ ìˆœ ì •ë ¬\n",
    "\n",
    "# ì´ˆë°˜ 2ì´ˆì˜ ë°ì´í„°ë¥¼ ì‚­ì œ\n",
    "start_time = df[\"Timestamp\"].iloc[0]\n",
    "df = df[df[\"Timestamp\"] > start_time + pd.Timedelta(seconds=2)]\n",
    "\n",
    "# PPG ì‹ í˜¸ ì²˜ë¦¬ (25Hz ê°€ì •)\n",
    "fs = 25  # ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜\n",
    "ppg_cleaned = nk.ppg_clean(df[\"PPG\"], sampling_rate=fs)\n",
    "\n",
    "# PPG í”¼í¬ ê²€ì¶œ\n",
    "peaks, info = nk.ppg_peaks(ppg_cleaned, sampling_rate=fs)\n",
    "\n",
    "# peaks ë°ì´í„°í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ë¥¼ dfì™€ ì¼ì¹˜ì‹œí‚¤ê¸°\n",
    "peaks[\"Timestamp\"] = df[\"Timestamp\"].reset_index(drop=True)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df[\"Timestamp\"], ppg_cleaned, label=\"Cleaned PPG\", color=\"blue\")\n",
    "plt.scatter(peaks[\"Timestamp\"][peaks[\"PPG_Peaks\"] == 1], ppg_cleaned[peaks[\"PPG_Peaks\"] == 1], color=\"red\", label=\"Peaks\", zorder=3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"PPG Signal\")\n",
    "plt.title(\"PPG Signal with Detected Peaks\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
